📰 뉴스 크롤러 & 트렌드 분석기 (Java Backend + Kafka)  

📌 프로젝트 소개  
뉴스 크롤러 & 트렌드 분석기는 Java(Spring Boot) 기반 웹 크롤링 시스템입니다.
  - Kafka를 활용해 URL 큐 관리 → 여러 Worker가 구독하여 뉴스 페이지 수집
  - Jsoup으로 HTML 파싱 → 기사 제목, 본문, 링크 추출
  - Spring Scheduler로 주기적으로 새로운 뉴스 URL 큐 생성
  - 수집된 뉴스는 DB에 저장 후 NLP 기반 키워드 추출, 요약, 트렌드 분석
  - REST API로 데이터를 제공

  
포인트:
Kafka 기반 URL 큐 + Worker 구조 → 확장성과 안정성 강조
뉴스 수집 → 분석 → API 제공까지 엔드투엔드 백엔드 프로젝트


📊 시스템 아키텍처
[뉴스 사이트] 
      │
      ▼
[URL Producer] (Spring Scheduler)
  - 새로운 뉴스 URL Kafka 토픽에 발행
      │
      ▼
┌───────────────────┐
│ Kafka Topic (URL 큐)│
└───────────────────┘
      │
      ▼
┌───────────────┐      ┌───────────────┐
│ Worker 1      │      │ Worker 2      │
│ - Kafka 구독  │      │ - Kafka 구독  │
│ - URL 수신    │      │ - URL 수신    │
│ - 뉴스 페이지 │      │ - 뉴스 페이지 │
│   크롤링      │      │   크롤링      │
│ - HTML 파싱   │      │ - HTML 파싱   │
└───────┬───────┘      └───────┬───────┘
        │                     │
        ▼                     ▼
                 ┌─────────────────┐
                 │ NLP 분석 모듈    │
                 │ - 키워드 추출    │
                 │ - 뉴스 요약      │
                 └─────────┬───────┘
                           │
                           ▼
                 ┌─────────────────┐
                 │ DB 저장소        │
                 │ - PostgreSQL    │
                 │ - MongoDB       │
                 │ - 기사 메타/본문 │
                 └─────────┬───────┘
                           │
                           ▼
                 ┌─────────────────┐
                 │ REST API 제공    │
                 │ - /news/search   │
                 │ - /trends        │
                 │ - /summary       │
                 └─────────────────┘

🚀 MVP 기능
1. 뉴스 기사 크롤링 기능
  - Kafka 기반 URL 큐 + Worker 구조
  - Spring Scheduler로 주기적 URL 발행
  - 뉴스 페이지 HTML 파싱 → 제목, 본문, 링크 추출
  1-1. 뉴스 기사 크롤링, 링크 검색
  - 링크만 모아서 재귀적 크롤링
    - 보통 메인에 주요기사들이있고 상세 기사 사이드쪽에 추천기사 등 사이드 기사들이 있음 
    - url 큐에 넣는 메인 에이전트는 메인에 주요 기사들을 추출해서 url을 카프카에 전송
    - 각 크롤러 워커들은 url 큐에서 url을 받고 메인기사, 댓글, 조회수 등 정보 추출을 하고 사이드에 기사들이있을경우
    - 사이드 기사들을 추출해서 url 큐에 다시 전송
    - Bloom Filter 및 depth를 설정하여 무한 크롤링 방지
  - 메인기사 json sample
  - 
2. 수집된 뉴스 저장 및 검색 API
  - DB에 기사 저장
  - 제목/키워드 기반 검색 REST API 제공

3. 뉴스 키워드 추출 및 빈도 분석
  - TF-IDF, TextRank 기반 키워드 추출
  - 키워드 발생 빈도 계산 → 트렌드 분석
4. 주간 트렌드 API 제공
  - 최근 7일/30일 키워드 트렌드 조회
  - REST API 제공
5. 키워드 기반 뉴스 요약 API
  - 특정 키워드 관련 기사 요약 제공
  - JSON 형태 반환
🛠 기술 스택
  - 언어 : Java 21
  - 프레임워크 : Spring Boot
  - 웹 스크래핑 : Jsoup
  - 스케줄링 : Spring Scheduler (@Scheduled)
  - NLP : OpenNLP
  - DB : PostgreSQL
  - 빌드 툴 : Gradle

📌 흐름 정리
1️⃣ A 서버 (URL Producer)

  - 특정 사이트의 초기 URL(seed URL)에서 첫 페이지 HTML을 요청
  - HTML에서 페이지 내 링크를 추출
  - 새 뉴스 페이지 URL, 관련 기사 URL 등
  - 추출한 URL을 Kafka 토픽에 메시지로 발행
  - 주기적으로 실행 가능 (Spring Scheduler 사용)
2️⃣ 워커 (Worker / URL Consumer)
  - Kafka 토픽을 구독 → 메시지(URL) 수신 받은 URL에 대해 두 가지 작업 중 하나 수행
    1. 링크 검사 / 추가 탐색
      - 해당 페이지에 또 다른 링크가 있는지 확인
      - 새로운 URL이 있으면 Kafka에 다시 발행 → 재귀적 크롤링
    2. 데이터 추출 (스크래핑)
      - 기사 제목, 본문, 날짜, 이미지 등 필요한 데이터 수집
  - 워커는 이 과정을 반복 → 큐에서 다음 URL 가져오기 → 크롤링 지속

🔑 핵심 포인트
  - Producer = URL 발행자 (초기 URL 수집 + 새 링크 발행)
  - Worker = URL 소비자 + 데이터 수집자 (링크 탐색 또는 데이터 추출)
  - Kafka가 URL 큐 역할 → 워커 여러 개로 분산 처리 가능
  - 한 페이지에서 링크가 있으면 다시 큐에 넣고, 없으면 데이터 추출만 수행
